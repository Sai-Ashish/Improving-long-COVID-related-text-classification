{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095c8f66513b462a9b321405d25e6e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700e3959a3e3481f927a599cae483bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/784k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3dda281c9c456e9e9eda4298ba5c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cae68b615ef42e6b3d5355a473ef9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28246aead0e044d1bcadff00881372e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9e47758c034e628fc5078cf076f433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the name of the first model\n",
    "first_model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "\n",
    "# Get the tokenizer\n",
    "first_model_tkn = MarianTokenizer.from_pretrained(first_model_name)\n",
    "\n",
    "# Load the pretrained model based on the name\n",
    "first_model = MarianMTModel.from_pretrained(first_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f82b4db24c0433ead14b7b7b0382833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/784k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2a456d5b60472487d5a488b0d92f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02e49d066b64f77800d3cee6044e644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a475c82cbff4bc7826b1d983b6f0c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082106d9a7b2445aad48617ac25b1e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e47d6fc96646dd9de517afa0c4edcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the name of the second model\n",
    "second_model_name = 'Helsinki-NLP/opus-mt-fr-en'\n",
    "\n",
    "# Get the tokenizer\n",
    "second_model_tkn = MarianTokenizer.from_pretrained(second_model_name)\n",
    "\n",
    "# Load the pretrained model based on the name\n",
    "second_model = MarianMTModel.from_pretrained(second_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_batch_texts(language_code, batch_texts):\n",
    "  \n",
    "    formated_bach = [\">>{}<< {}\".format(language_code, text) for text in batch_texts]\n",
    "\n",
    "    return formated_bach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_name, tokenizer=first_model_tkn, language='fr'):\n",
    "    if dataset_name == 'covid':\n",
    "        # Load the dataset into a pandas dataframe.\n",
    "        dataset = load_dataset('llangnickel/long-covid-classification-data')\n",
    "        # get the training data\n",
    "        batch = dataset['train']['text']\n",
    "        labels = dataset['train']['label']\n",
    "        \n",
    "    elif dataset_name == 'cancer':\n",
    "        classes = {'Thyroid_Cancer' : 0,  'Lung_Cancer' : 1,  'Colon_Cancer' : 2}\n",
    "        # Load the dataset into a pandas dataframe.\n",
    "        df = pd.read_csv('../../cancer.csv', encoding='latin')\n",
    "        values = df.values\n",
    "\n",
    "        # get the entire data\n",
    "        all_batch  = list(values[:,2])\n",
    "        str_labels = list(values[:,1])\n",
    "        all_labels = [classes[k] for k in str_labels]\n",
    "\n",
    "        # training and test data split\n",
    "        batch, _, labels, _ = train_test_split(all_batch, all_labels, test_size=0.5, random_state=42)\n",
    "        batch = batch[:100] # since it is an easy dataset use just 500 data points\n",
    "        labels = labels[:100]\n",
    "    \n",
    "    elif dataset_name == 'medical_texts':\n",
    "        \n",
    "        f = open('../../Medical texts/train.dat', 'r')\n",
    "        lines = f.readlines()\n",
    "        batch = list()\n",
    "        labels = list()\n",
    "        for line in lines:\n",
    "            labels.append(int(line[0])-1) # subtract 1 to make it in the range required by the model\n",
    "            batch.append(line[2:len(line)-1])\n",
    "        f.close()\n",
    "\n",
    "        # training and test data split\n",
    "        batch, test_batch, labels, test_labels = train_test_split(batch, labels, test_size=0.2, random_state=42)\n",
    "        \n",
    "        batch = batch[:500] # since it is an easy dataset use just 500 data points\n",
    "        labels = labels[:500]\n",
    "    \n",
    "    # Prepare the text data into appropriate format for the model\n",
    "    original_texts = batch\n",
    "    original_labels = labels\n",
    "    batch = format_batch_texts(language, batch)\n",
    "    \n",
    "    # shuffle the data\n",
    "    data = list(zip(batch, labels))\n",
    "    random.shuffle(data)\n",
    "    train_batch, labels = zip(*data)\n",
    "    train_labels = torch.tensor(labels)\n",
    "    \n",
    "    # tokenizing the sentences\n",
    "    seq_length = 128\n",
    "    encoding = tokenizer(train_batch, return_tensors='pt', padding=True, truncation = True, max_length=seq_length)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    \n",
    "    # print the shapes\n",
    "    print(\"Input shape: \")\n",
    "    print(input_ids.shape, attention_mask.shape,train_labels.shape,train_labels)\n",
    "    \n",
    "    # turn to the tensordataset\n",
    "    train_data = TensorDataset(input_ids, attention_mask, train_labels)\n",
    "        \n",
    "    return original_texts, original_labels, train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: \n",
      "torch.Size([500, 128]) torch.Size([500, 128]) torch.Size([500]) tensor([4, 4, 0, 1, 1, 2, 4, 2, 3, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 4, 4, 4,\n",
      "        3, 4, 2, 4, 4, 4, 2, 4, 4, 3, 4, 4, 3, 2, 4, 4, 0, 2, 3, 0, 2, 3, 1, 4,\n",
      "        3, 4, 4, 0, 1, 3, 4, 0, 1, 4, 0, 2, 4, 0, 0, 2, 2, 0, 3, 4, 4, 2, 1, 2,\n",
      "        4, 1, 4, 1, 4, 3, 4, 4, 3, 0, 3, 0, 4, 3, 4, 4, 3, 4, 4, 0, 3, 0, 3, 0,\n",
      "        2, 4, 0, 3, 1, 2, 0, 2, 3, 0, 4, 4, 0, 4, 4, 0, 3, 3, 4, 4, 4, 0, 0, 2,\n",
      "        4, 2, 4, 0, 4, 4, 4, 3, 3, 0, 3, 3, 4, 4, 4, 3, 3, 4, 2, 4, 4, 3, 4, 0,\n",
      "        4, 3, 4, 4, 3, 2, 4, 2, 1, 4, 4, 2, 4, 4, 4, 4, 3, 4, 3, 0, 3, 1, 4, 0,\n",
      "        0, 2, 3, 4, 4, 4, 2, 1, 0, 1, 3, 3, 4, 3, 3, 0, 4, 3, 4, 4, 4, 2, 2, 3,\n",
      "        2, 4, 2, 2, 0, 4, 1, 3, 4, 1, 3, 3, 2, 4, 0, 3, 1, 3, 4, 4, 4, 3, 3, 2,\n",
      "        2, 3, 0, 4, 4, 0, 0, 0, 3, 0, 0, 4, 0, 4, 3, 3, 1, 4, 2, 4, 3, 0, 3, 3,\n",
      "        4, 1, 1, 4, 0, 1, 4, 4, 3, 0, 4, 0, 4, 4, 4, 2, 0, 3, 4, 4, 4, 0, 4, 0,\n",
      "        2, 2, 3, 3, 4, 0, 4, 1, 0, 0, 2, 0, 0, 1, 0, 4, 0, 0, 4, 3, 2, 4, 1, 0,\n",
      "        4, 3, 4, 1, 4, 4, 0, 0, 4, 2, 2, 1, 2, 3, 0, 0, 3, 3, 3, 1, 0, 4, 0, 3,\n",
      "        2, 3, 2, 0, 1, 4, 2, 4, 4, 4, 4, 3, 4, 3, 3, 0, 2, 0, 3, 3, 1, 0, 4, 0,\n",
      "        3, 1, 4, 2, 0, 2, 1, 2, 2, 3, 4, 0, 4, 4, 0, 4, 3, 2, 2, 3, 0, 4, 3, 3,\n",
      "        3, 1, 3, 0, 2, 0, 4, 4, 1, 0, 4, 4, 4, 4, 2, 2, 4, 2, 3, 4, 1, 4, 3, 4,\n",
      "        3, 0, 4, 0, 4, 3, 0, 4, 0, 0, 3, 2, 4, 4, 2, 4, 4, 2, 4, 3, 4, 4, 4, 4,\n",
      "        4, 2, 3, 4, 0, 4, 4, 0, 0, 4, 1, 3, 0, 1, 0, 4, 0, 0, 1, 3, 1, 2, 0, 0,\n",
      "        3, 4, 3, 0, 1, 0, 2, 2, 1, 4, 4, 0, 2, 3, 4, 4, 0, 0, 1, 2, 0, 3, 4, 3,\n",
      "        4, 2, 3, 2, 3, 1, 3, 3, 1, 0, 4, 2, 4, 2, 4, 4, 4, 3, 1, 3, 3, 0, 2, 3,\n",
      "        0, 0, 3, 1, 0, 0, 4, 0, 3, 3, 4, 0, 2, 0, 4, 1, 0, 0, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "dataset = 'medical_texts'\n",
    "original_texts, original_labels, train_data_classifier = get_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classifier = DataLoader(train_data_classifier, batch_size=16, sampler=torch.utils.data.sampler.RandomSampler(train_data_classifier), pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_translation_forward(dataloader, model, tokenizer):\n",
    "    \n",
    "    translated_texts = []\n",
    "    \n",
    "    aug_labels = []\n",
    "    \n",
    "    for step, batch in enumerate(dataloader):\n",
    "        \n",
    "        print(\"At batch : \" + str(step))\n",
    "        \n",
    "        translated = model.generate(batch[0].to(device))\n",
    "        \n",
    "        batch_translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        \n",
    "        translated_texts = translated_texts + batch_translated_texts\n",
    "        \n",
    "        aug_labels = aug_labels + batch[2].tolist()\n",
    "        \n",
    "    return translated_texts, aug_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At batch : 0\n",
      "At batch : 1\n",
      "At batch : 2\n",
      "At batch : 3\n",
      "At batch : 4\n",
      "At batch : 5\n",
      "At batch : 6\n",
      "At batch : 7\n",
      "At batch : 8\n",
      "At batch : 9\n",
      "At batch : 10\n",
      "At batch : 11\n",
      "At batch : 12\n",
      "At batch : 13\n",
      "At batch : 14\n",
      "At batch : 15\n",
      "At batch : 16\n",
      "At batch : 17\n",
      "At batch : 18\n",
      "At batch : 19\n",
      "At batch : 20\n",
      "At batch : 21\n",
      "At batch : 22\n",
      "At batch : 23\n",
      "At batch : 24\n",
      "At batch : 25\n",
      "At batch : 26\n",
      "At batch : 27\n",
      "At batch : 28\n",
      "At batch : 29\n",
      "At batch : 30\n",
      "At batch : 31\n"
     ]
    }
   ],
   "source": [
    "translated_texts, aug_labels = perform_translation_forward(train_classifier, first_model, first_model_tkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Le traitement par reperfusion a été clairement démontré pour diminuer la mortalité précoce après un infarctus aigu du myocarde, mais l'impact de ce traitement sur la survie à long terme a été moins largement évalué. Cette étude rapporte le suivi prolongé d'une grande cohorte de 810 patients traités par un traitement thrombolytique par voie intraveineuse combinés, lorsqu'il est jugé nécessaire de maintenir ou d'augmenter la patiesse du vaisseau infarctus, avec des traitements de reperfusion mécanique.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_translation_backward(translated_texts, model, tokenizer, language='en'):\n",
    "    \n",
    "    # Prepare the text data into appropriate format for the model\n",
    "    formated_batch_texts = format_batch_texts(language, translated_texts)\n",
    "    \n",
    "    translated_texts = []\n",
    "    \n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in range(0, len(formated_batch_texts), batch_size):\n",
    "        \n",
    "        print(\"At batch : \" + str(i//batch_size))\n",
    "        \n",
    "        encoding = tokenizer(formated_batch_texts[i:i+batch_size], return_tensors=\"pt\", padding=True, truncation = True, max_length=128)\n",
    "    \n",
    "        # Generate translation using model\n",
    "        translated = model.generate(encoding['input_ids'].to(device))\n",
    "        \n",
    "        # Convert the generated tokens indices back into text\n",
    "        batch_translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        \n",
    "        translated_texts = translated_texts + batch_translated_texts\n",
    "    \n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At batch : 0\n",
      "At batch : 1\n",
      "At batch : 2\n",
      "At batch : 3\n",
      "At batch : 4\n",
      "At batch : 5\n",
      "At batch : 6\n",
      "At batch : 7\n",
      "At batch : 8\n",
      "At batch : 9\n",
      "At batch : 10\n",
      "At batch : 11\n",
      "At batch : 12\n",
      "At batch : 13\n",
      "At batch : 14\n",
      "At batch : 15\n",
      "At batch : 16\n",
      "At batch : 17\n",
      "At batch : 18\n",
      "At batch : 19\n",
      "At batch : 20\n",
      "At batch : 21\n",
      "At batch : 22\n",
      "At batch : 23\n",
      "At batch : 24\n",
      "At batch : 25\n",
      "At batch : 26\n",
      "At batch : 27\n",
      "At batch : 28\n",
      "At batch : 29\n",
      "At batch : 30\n",
      "At batch : 31\n"
     ]
    }
   ],
   "source": [
    "back_translated_texts = perform_translation_backward(translated_texts, second_model, second_model_tkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(back_translated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Data/backtranslation_\"+str(dataset)+\".txt\" , \"a\" )\n",
    "\n",
    "for i in range(len(back_translated_texts)):\n",
    "    f.write(str(aug_labels[i]) + '\\t' + back_translated_texts[i] + '\\n')\n",
    "    \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
