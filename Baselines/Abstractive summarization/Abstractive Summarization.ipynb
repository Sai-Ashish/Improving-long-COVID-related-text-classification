{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4f6e63d3e546f3b5995fe33937e319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00000afcd6b34020aea01b60ac514e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c39dc26d8e4ecb8c78f9e9f92bd6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5_fast.py:161: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922868e63ca24a79ae6ab1e3d9d09d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "aug = nas.AbstSummAug(model_path='t5-large', tokenizer_path='t5-large', max_length=185, batch_size=16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_name):\n",
    "    if dataset_name == 'covid':\n",
    "        # Load the dataset into a pandas dataframe.\n",
    "        dataset = load_dataset('llangnickel/long-covid-classification-data')\n",
    "        # get the training data\n",
    "        batch = dataset['train']['text']\n",
    "        labels = dataset['train']['label']\n",
    "        \n",
    "    elif dataset_name == 'cancer':\n",
    "        classes = {'Thyroid_Cancer' : 0,  'Lung_Cancer' : 1,  'Colon_Cancer' : 2}\n",
    "        # Load the dataset into a pandas dataframe.\n",
    "        df = pd.read_csv('../../cancer.csv', encoding='latin')\n",
    "        values = df.values\n",
    "\n",
    "        # get the entire data\n",
    "        all_batch  = list(values[:,2])\n",
    "        str_labels = list(values[:,1])\n",
    "        all_labels = [classes[k] for k in str_labels]\n",
    "\n",
    "        # training and test data split\n",
    "        batch, _, labels, _ = train_test_split(all_batch, all_labels, test_size=0.5, random_state=42)\n",
    "        batch = batch[:100] # since it is an easy dataset use just 500 data points\n",
    "        labels = labels[:100]\n",
    "    \n",
    "    elif dataset_name == 'medical_texts':\n",
    "        \n",
    "        f = open('../../Medical texts/train.dat', 'r')\n",
    "        lines = f.readlines()\n",
    "        batch = list()\n",
    "        labels = list()\n",
    "        for line in lines:\n",
    "            labels.append(int(line[0])-1) # subtract 1 to make it in the range required by the model\n",
    "            batch.append(line[2:len(line)-1])\n",
    "        f.close()\n",
    "\n",
    "        # training and test data split\n",
    "        batch, test_batch, labels, test_labels = train_test_split(batch, labels, test_size=0.2, random_state=42)\n",
    "        \n",
    "        batch = batch[:500] # since it is an easy dataset use just 500 data points\n",
    "        labels = labels[:500]\n",
    "            \n",
    "    elif dataset_name == 'medical_trans':\n",
    "        \n",
    "        label_dict = {' Neurology': 0,\n",
    "                     ' Discharge Summary': 1,\n",
    "                     ' Psychiatry / Psychology': 2,\n",
    "                     ' Pediatrics - Neonatal': 3,\n",
    "                     ' Neurosurgery': 4,\n",
    "                     ' Gastroenterology': 5,\n",
    "                     ' Emergency Room Reports': 6,\n",
    "                     ' Ophthalmology': 7,\n",
    "                     ' Office Notes': 8,\n",
    "                     ' Cardiovascular / Pulmonary': 9,\n",
    "                     ' Nephrology': 10,\n",
    "                     ' ENT - Otolaryngology': 11,\n",
    "                     ' Hematology - Oncology': 12,\n",
    "                     ' Obstetrics / Gynecology': 13,\n",
    "                     ' Orthopedic': 14,\n",
    "                     ' Urology': 15,\n",
    "                     ' Radiology': 16,\n",
    "                     ' General Medicine': 17,\n",
    "                     ' Pain Management': 18,\n",
    "                     ' SOAP / Chart / Progress Notes': 19,\n",
    "                     ' Consult - History and Phy.': 20,\n",
    "                     ' Surgery': 21}\n",
    "        \n",
    "        f = open('medical_transcriptions.txt', 'r')\n",
    "        lines = f.readlines()\n",
    "        # get the training data\n",
    "        all_batch = []\n",
    "        all_labels = []\n",
    "        for line in lines:\n",
    "            parts = line.split('\\t')\n",
    "            batch_label = label_dict[parts[0]]\n",
    "            if batch_label != 20 and batch_label != 21: # if included these labels there will be huge data imbalance\n",
    "                all_labels.append(batch_label)\n",
    "                all_batch.append(parts[1])\n",
    "        f.close()\n",
    "        \n",
    "        # training and test data split\n",
    "        batch, test_batch, labels, test_labels = train_test_split(all_batch, all_labels, test_size=0.835, random_state=42)\n",
    "        \n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'medical_trans'\n",
    "original_texts, original_labels = get_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_texts), len(original_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_texts = []\n",
    "for i in range(len(original_texts)):\n",
    "    truncated_texts.append(' '.join(original_texts[i].split()[:256]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "augmented_data = []\n",
    "\n",
    "for i in range(len(truncated_texts)):\n",
    "    augmented_data.append(aug.augment(truncated_texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Data/abstractivesummarization_\"+str(dataset)+\".txt\" , \"a\" )\n",
    "\n",
    "for i in range(len(augmented_data)):\n",
    "    f.write(str(original_labels[i]) + '\\t' + augmented_data[i][0] + '\\n')\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hexamethylene bisacetamide is a potent polar-planar differentiating agent of leukemia and solid tumor cell lines in vitro at clinically achievable concentrations. HMBA produced nearly identical inhibition of normal and myelodysplastic hematopoietic progenitors.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_data[10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_labels[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
